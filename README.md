This repository contains the code for the paper "How and Why are We Discovering False Denial Constraints".

--------------------

## Directory guide

* ```algorithmJARs/```: Contains algorithms compiled with all dependencies (FatJars). 
* ```algorithms/```: Contains the source codes of the algorithms, modified to accept a common input format. 
* ```datasets/```: Contains the raw .csv data files. 
* ```goldenDCs/```: Contains the sets of golden DCs for each dataset generated manually by domain experts. 
* ```results/```: Saved intermediate results of the experiments. More details below. 
* ```soundDCs/```: Contains the sets of sound DCs for each dataset discovered with our method. 
* ```results.ipynb```: Jupyter notebook containing all code with the experiments and notes to reproduce the results. 
--------------------

## Reproducibility

The process of reproducing the experiments is divided in three distinct stages. Each of them is optional as its results are packaged in the repository.

* **Algorithm compilation**
* **Experiment execution**
* **Figure generation**

--------------------

### Algorithm compilation

This stage compiles the algorithms in ```./algorithms``` into the jar files to be used during the experiments in ```./algorithmJARs```. It is strongly recommended to skip this step as it takes a long time. The repository contains the packaged executables already ready for use in ```./algorithmJARs```.
#### Dependencies
* Java JDK 1.8 or later
* Maven 3.1.0 or later
* Git
* Having installed Metanome, following the instructions at https://github.com/HPI-Information-Systems/Metanome/tree/master

#### Compilation

Algorithms may be compiled running the following command from their root directory:

```mvn clean package```

After the process finishes, the executable is found in the ```target``` directory of the algorithm.

#### Usage

Algorithms may be executed from console with the following command.

```java -jar ALGORITHM DATASET APPROX ROWS```

Where each parameter is:

* ALGORITHM: path to the algorithm jar.
* DATASET: path to the dataset.
* APPROX: approximation factor from 0 to 1. Values used in the paper are 0 (exact discovery), 0.000001, 0.0001, 0.01. Only used by algorithms that allow approximation.
* ROWS: Number of rows of the dataset to be used. The paper used 15000, meaning algorithms worked with the first 15000 rows of each dataset.

#### Example

Starting at the root directory, in order to compile FastDC one must first move to the directory of this algorithm:

```cd algorithms/FastDC```

Then the algorithm must be compiled.

```mvn clean package```

Which will generate the executable in the ```algorithms/FastDC/target``` directory. It can be moved to ```./algorithmJARs``` with:

``` mv ./target/FastDC-1.2-SNAPSHOT.jar ../../algorithmJARs/FastDC.jar```

From the root directory, it can be executed on the first 15000 rows of the tax dataset with no approximation with the following command:

```java -jar algorithmJARs/FastDC.jar datasets/tax500k.csv 0.00 15000```

And will generate a file named ```output.txt``` with the resulting DCs.

--------------------

### Experiment execution

This stage uses the compiled algorithms to generate the data necessary for the experiments. All code required is in ```results.ipynb```.

#### Dependencies
* Python 3.9 or above

#### Execution

There are five distinct experiments, building upon each other but in separate code blocks. The results of each are packaged along with the repository in the ```results/``` directory, so each step is optional:

* **Discovering DCs**: This step executes the compiled algorithms with multiple comfigurations and stores the results
* **Computing DC satisfactions**: This step computes the satisfaction of each discovered DC, as well as several of their subsets (for the experiments, not for the plots. More details in the code).
* **Computing node satisfactions**: This step computes the satisfaction of all sets of predicates below a certain size.
* **Discovering all atomic (sound) sets of predicates**: This step computes all predicate sets with statistically significant atomicity (provably sound DCs).
* **Analyzing evolution as data is increased**: This step discovers DCs several times with increasing tuple count in order to assess how the results evolve as more data is available.
  
Further details are provided in the notebook.

--------------------

### Figure generation

This final step uses the results generated by the experiments to generate the LaTeX TikZ code for the figures. There are several cells, each containing the functions to generate one of the figures found in the paper. Once the functions are defined, a final cell executes them and prints the code, optionally generating a preview.

